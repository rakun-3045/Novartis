{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f08264f",
   "metadata": {},
   "source": [
    "# Clinical Trial RAG Pipeline - Optimized\n",
    "\n",
    "## Overview\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) pipeline for clinical trial data analysis.\n",
    "\n",
    "### RAG-Ready Files in `consolidated_data/`:\n",
    "| File | Description | Records | Source |\n",
    "|------|-------------|---------|--------|\n",
    "| `rag_combined_documents.jsonl` | **Master document store** - combines all document types | ~31,500 | Generated from all other RAG files |\n",
    "| `rag_subject_documents.jsonl` | Subject-level profiles with risk and issues | ~28,900 | Generated from `global_clinical_data.csv` |\n",
    "| `rag_study_documents.jsonl` | Study-level summaries | 24 | Aggregated from subject data |\n",
    "| `rag_site_documents.jsonl` | Site performance reports | ~2,500 | Grouped by Study + Site |\n",
    "| `rag_dqi_documents.jsonl` | Data Quality Index (DQI) scores per subject | ~29,400 | Calculated from issue metrics |\n",
    "| `rag_study_dqi_summaries.jsonl` | Study-level DQI summaries | 24 | Aggregated DQI scores |\n",
    "| `rag_cra_reports.jsonl` | CRA Monitoring Reports per study | 24 | Executive summaries |\n",
    "| `rag_data_dictionary.md` | Field definitions & terminology | 1 | Manual reference |\n",
    "\n",
    "### Document Generation Logic (from `tests.ipynb`):\n",
    "- **Subject Documents**: Natural language profiles generated from `global_df` with risk assessment, issue summaries, and CRA action items\n",
    "- **Study Documents**: Aggregated metrics (risk distribution, issue breakdown, priority subjects)\n",
    "- **Site Documents**: Site-level performance metrics grouped by Study + Site\n",
    "- **DQI Documents**: Calculated scores based on issue types (-2 per open issue, -1 per missing lab, etc.)\n",
    "- **CRA Reports**: Executive summaries with priority actions\n",
    "\n",
    "### Key Challenge: Study Data Imbalance\n",
    "Studies have vastly different subject counts (Study 16: 672 subjects vs Study 14: 3 subjects).\n",
    "This pipeline uses **weighted sampling and MMR** to ensure balanced retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e89dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Install Dependencies\n",
    "# ============================================================================\n",
    "# Run this cell only once to install required packages\n",
    "\n",
    "!pip install langchain langchain-huggingface langchain-community faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized: Gemma 27B\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Setup LLM (Gemma 27B via HuggingFace Inference API)\n",
    "# ============================================================================\n",
    "import os\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "# Set your HuggingFace token here or as environment variable\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_your_token_here\"\n",
    "if not os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
    "    print(\"‚ö†Ô∏è Please set HUGGINGFACEHUB_API_TOKEN environment variable\")\n",
    "\n",
    "# Gemma 27B for generation - optimized for conversational responses\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-3-27b-it\",\n",
    "    temperature=0.4,\n",
    "    max_new_tokens=2048,  # Increased for longer, more detailed responses\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "print(\"‚úÖ LLM initialized: Gemma 27B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb96c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings initialized: all-MiniLM-L6-v2\n",
      "   Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Setup Embeddings (all-MiniLM-L6-v2 - lightweight & fast)\n",
    "# ============================================================================\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# all-MiniLM-L6-v2: 384-dimensional embeddings, optimized for semantic similarity\n",
    "# Much faster than Gemma 300M while maintaining good quality\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={\n",
    "        'normalize_embeddings': True,  # Normalize for cosine similarity\n",
    "        'batch_size': 64  # Larger batches for faster processing\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embeddings.embed_query(\"test clinical trial data\")\n",
    "print(f\"‚úÖ Embeddings initialized: all-MiniLM-L6-v2\")\n",
    "print(f\"   Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8dc7ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing RAG document distribution...\n",
      "\n",
      "Document counts by type:\n",
      "  ‚Ä¢ subject_dqi: 29,376\n",
      "  ‚Ä¢ subject_profile: 28,904\n",
      "  ‚Ä¢ site_summary: 2,557\n",
      "  ‚Ä¢ study_summary: 23\n",
      "  ‚Ä¢ cra_report: 23\n",
      "  ‚Ä¢ study_dqi: 23\n",
      "\n",
      "üìà Study distribution (top 5 by document count):\n",
      "  ‚Ä¢ Study 21: 21,459 documents\n",
      "  ‚Ä¢ STUDY 21: 20,869 documents\n",
      "  ‚Ä¢ Study 25: 2,976 documents\n",
      "  ‚Ä¢ Study 4: 2,827 documents\n",
      "  ‚Ä¢ Study 23: 1,892 documents\n",
      "\n",
      "‚ö†Ô∏è  Data imbalance detected: Study 16 has 1,523 docs vs Study 14 has 11 docs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Load and Analyze RAG Documents\n",
    "# ============================================================================\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Document store paths with descriptions\n",
    "RAG_FILES = {\n",
    "    \"rag_study_documents.jsonl\": {\n",
    "        \"description\": \"Study-level summaries (highest priority)\",\n",
    "        \"doc_type\": \"study_summary\",\n",
    "        \"priority\": 1\n",
    "    },\n",
    "    \"rag_cra_reports.jsonl\": {\n",
    "        \"description\": \"CRA monitoring reports\",\n",
    "        \"doc_type\": \"cra_report\",\n",
    "        \"priority\": 1\n",
    "    },\n",
    "    \"rag_study_dqi_summaries.jsonl\": {\n",
    "        \"description\": \"Study DQI summaries\",\n",
    "        \"doc_type\": \"study_dqi\",\n",
    "        \"priority\": 2\n",
    "    },\n",
    "    \"rag_site_documents.jsonl\": {\n",
    "        \"description\": \"Site performance reports\",\n",
    "        \"doc_type\": \"site_summary\",\n",
    "        \"priority\": 2\n",
    "    },\n",
    "    \"rag_dqi_documents.jsonl\": {\n",
    "        \"description\": \"Subject DQI scores\",\n",
    "        \"doc_type\": \"subject_dqi\",\n",
    "        \"priority\": 3\n",
    "    },\n",
    "    \"rag_subject_documents.jsonl\": {\n",
    "        \"description\": \"Subject profiles\",\n",
    "        \"doc_type\": \"subject_profile\",\n",
    "        \"priority\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "BASE_PATH = \"consolidated_data\"\n",
    "\n",
    "# Analyze document distribution\n",
    "print(\"üìä Analyzing RAG document distribution...\\n\")\n",
    "study_doc_counts = defaultdict(lambda: defaultdict(int))\n",
    "total_by_type = defaultdict(int)\n",
    "\n",
    "for filename, config in RAG_FILES.items():\n",
    "    filepath = os.path.join(BASE_PATH, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    study = data.get('study', 'Unknown')\n",
    "                    study_doc_counts[study][config['doc_type']] += 1\n",
    "                    total_by_type[config['doc_type']] += 1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "print(\"Document counts by type:\")\n",
    "for doc_type, count in sorted(total_by_type.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  ‚Ä¢ {doc_type}: {count:,}\")\n",
    "\n",
    "print(\"\\nüìà Study distribution (top 5 by document count):\")\n",
    "study_totals = {study: sum(counts.values()) for study, counts in study_doc_counts.items()}\n",
    "for study, total in sorted(study_totals.items(), key=lambda x: -x[1])[:5]:\n",
    "    print(f\"  ‚Ä¢ {study}: {total:,} documents\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Data imbalance detected: Study 16 has {study_totals.get('Study 16', 0):,} docs vs Study 14 has {study_totals.get('Study 14', 0):,} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4396ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loaded 5,882 documents with stratified sampling:\n",
      "\n",
      "  ‚Ä¢ site_summary: 2,557\n",
      "  ‚Ä¢ subject_dqi: 1,630\n",
      "  ‚Ä¢ subject_profile: 1,625\n",
      "  ‚Ä¢ study_summary: 23\n",
      "  ‚Ä¢ cra_report: 23\n",
      "  ‚Ä¢ study_dqi: 23\n",
      "  ‚Ä¢ data_dictionary: 1\n",
      "\n",
      "üìä Study distribution after sampling (top 5):\n",
      "  ‚Ä¢ Study 21: 1158 documents\n",
      "  ‚Ä¢ Study 22: 481 documents\n",
      "  ‚Ä¢ Study 25: 428 documents\n",
      "  ‚Ä¢ Study 16: 379 documents\n",
      "  ‚Ä¢ Study 4: 371 documents\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Load Documents with Stratified Sampling Strategy\n",
    "# ============================================================================\n",
    "# Strategy to handle data imbalance:\n",
    "# 1. Load ALL high-priority documents (study summaries, CRA reports) - they're few\n",
    "# 2. Load ALL site documents - moderate count\n",
    "# 3. Sample subject-level documents proportionally (max per study)\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "MAX_SUBJECTS_PER_STUDY = 100  # Cap subject docs per study to prevent dominance\n",
    "\n",
    "documents = []\n",
    "doc_counts = defaultdict(int)\n",
    "\n",
    "# Load data dictionary first (always include)\n",
    "data_dict_path = os.path.join(BASE_PATH, \"rag_data_dictionary.md\")\n",
    "if os.path.exists(data_dict_path):\n",
    "    with open(data_dict_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        documents.append(Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"source\": \"rag_data_dictionary.md\",\n",
    "                \"doc_type\": \"data_dictionary\",\n",
    "                \"study\": \"ALL\",\n",
    "                \"priority\": 0\n",
    "            }\n",
    "        ))\n",
    "        doc_counts[\"data_dictionary\"] += 1\n",
    "\n",
    "# Load documents by priority\n",
    "for filename, config in RAG_FILES.items():\n",
    "    filepath = os.path.join(BASE_PATH, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        continue\n",
    "    \n",
    "    # For subject-level docs, we'll sample\n",
    "    if config['priority'] == 3:  # Subject-level docs\n",
    "        # First pass: group by study\n",
    "        study_docs = defaultdict(list)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    content = data.get('content') or data.get('document', '')\n",
    "                    if not content:\n",
    "                        continue\n",
    "                    study = data.get('study', 'Unknown')\n",
    "                    study_docs[study].append((data, content))\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Sample from each study\n",
    "        for study, docs in study_docs.items():\n",
    "            # Prioritize high-risk subjects\n",
    "            sorted_docs = sorted(docs, key=lambda x: -x[0].get('total_issues', 0))\n",
    "            sampled = sorted_docs[:MAX_SUBJECTS_PER_STUDY]\n",
    "            \n",
    "            for data, content in sampled:\n",
    "                metadata = {k: v for k, v in data.items() if k not in ['content', 'document']}\n",
    "                metadata['source'] = filename\n",
    "                metadata['doc_type'] = config['doc_type']\n",
    "                metadata['priority'] = config['priority']\n",
    "                documents.append(Document(page_content=content, metadata=metadata))\n",
    "                doc_counts[config['doc_type']] += 1\n",
    "    else:\n",
    "        # Load all non-subject documents\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    content = data.get('content') or data.get('document', '')\n",
    "                    if not content:\n",
    "                        continue\n",
    "                    \n",
    "                    metadata = {k: v for k, v in data.items() if k not in ['content', 'document']}\n",
    "                    metadata['source'] = filename\n",
    "                    metadata['doc_type'] = config['doc_type']\n",
    "                    metadata['priority'] = config['priority']\n",
    "                    documents.append(Document(page_content=content, metadata=metadata))\n",
    "                    doc_counts[config['doc_type']] += 1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "print(f\"üìö Loaded {len(documents):,} documents with stratified sampling:\\n\")\n",
    "for doc_type, count in sorted(doc_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  ‚Ä¢ {doc_type}: {count:,}\")\n",
    "\n",
    "# Verify study balance after sampling\n",
    "study_counts = defaultdict(int)\n",
    "for doc in documents:\n",
    "    study_counts[doc.metadata.get('study', 'Unknown')] += 1\n",
    "\n",
    "print(f\"\\nüìä Study distribution after sampling (top 5):\")\n",
    "for study, count in sorted(study_counts.items(), key=lambda x: -x[1])[:5]:\n",
    "    print(f\"  ‚Ä¢ {study}: {count} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39cb716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Adding 5,882 documents to FAISS vector store...\n",
      "  Progress: 5,882/5,882 (100.0%)\n",
      "‚úÖ Vector store created with 5,882 documents\n",
      "üíæ Saved to: faiss_index_optimized/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Create FAISS Vector Store with Document Indexing\n",
    "# ============================================================================\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Initialize FAISS index\n",
    "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# Add documents in batches\n",
    "print(f\"üîÑ Adding {len(documents):,} documents to FAISS vector store...\")\n",
    "\n",
    "batch_size = 500  # Optimized for MiniLM\n",
    "total_docs = len(documents)\n",
    "\n",
    "for i in range(0, total_docs, batch_size):\n",
    "    batch = documents[i:i+batch_size]\n",
    "    vector_store.add_documents(batch)\n",
    "    \n",
    "    current = min(i + batch_size, total_docs)\n",
    "    pct = (current / total_docs) * 100\n",
    "    print(f\"  Progress: {current:,}/{total_docs:,} ({pct:.1f}%)\", end='\\r')\n",
    "\n",
    "print(f\"\\n‚úÖ Vector store created with {total_docs:,} documents\")\n",
    "\n",
    "# Save for future use\n",
    "VECTORSTORE_PATH = \"faiss_index_optimized\"\n",
    "vector_store.save_local(VECTORSTORE_PATH)\n",
    "print(f\"üíæ Saved to: {VECTORSTORE_PATH}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c30c5d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced retrieval function defined\n",
      "   Features:\n",
      "   ‚Ä¢ MMR for diversity across studies\n",
      "   ‚Ä¢ Priority-aware re-ranking (study summaries > subjects)\n",
      "   ‚Ä¢ Study balancing to prevent single-study dominance\n",
      "   ‚Ä¢ Optional study filtering\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Advanced Retrieval Strategy - Multi-Stage Retrieval\n",
    "# ============================================================================\n",
    "# Strategy to handle study imbalance and improve relevance:\n",
    "# 1. First retrieve by document priority (study summaries first)\n",
    "# 2. Use MMR for diversity across studies\n",
    "# 3. Apply post-retrieval re-ranking based on relevance + priority\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from collections import defaultdict\n",
    "\n",
    "def advanced_retrieve(question: str, k: int = 15, study_filter: str = None):\n",
    "    \"\"\"\n",
    "    Multi-stage retrieval with diversity and priority-aware re-ranking.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        k: Total number of documents to retrieve\n",
    "        study_filter: Optional study filter (e.g., \"Study 10\")\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant documents with diversity across studies\n",
    "    \"\"\"\n",
    "    # Stage 1: Get more candidates using MMR for diversity\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": k * 3,  # Over-fetch for re-ranking\n",
    "            \"fetch_k\": k * 5,  # Fetch even more for MMR diversity\n",
    "            \"lambda_mult\": 0.7  # Balance relevance (1.0) vs diversity (0.0)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    candidates = retriever.invoke(question)\n",
    "    \n",
    "    # Stage 2: Apply study filter if specified\n",
    "    if study_filter:\n",
    "        # Normalize study filter for matching\n",
    "        study_filter_normalized = study_filter.strip().lower()\n",
    "        filtered = [\n",
    "            doc for doc in candidates\n",
    "            if doc.metadata.get('study', '').lower().strip() == study_filter_normalized\n",
    "            or doc.metadata.get('study', '').lower().strip().replace('study ', '') == study_filter_normalized.replace('study ', '')\n",
    "        ]\n",
    "        if len(filtered) >= k // 2:\n",
    "            candidates = filtered\n",
    "    \n",
    "    # Stage 3: Priority-aware re-ranking\n",
    "    # Score = base_score + priority_boost\n",
    "    def get_priority_score(doc):\n",
    "        priority = doc.metadata.get('priority', 3)\n",
    "        doc_type = doc.metadata.get('doc_type', '')\n",
    "        \n",
    "        # Higher score = higher priority\n",
    "        priority_boost = {\n",
    "            0: 100,  # Data dictionary\n",
    "            1: 50,   # Study summaries, CRA reports\n",
    "            2: 25,   # Site docs, DQI summaries\n",
    "            3: 10    # Subject-level docs\n",
    "        }\n",
    "        \n",
    "        return priority_boost.get(priority, 0)\n",
    "    \n",
    "    # Sort by priority (stable sort preserves MMR ordering within priority)\n",
    "    candidates_sorted = sorted(candidates, key=get_priority_score, reverse=True)\n",
    "    \n",
    "    # Stage 4: Ensure study diversity in final results\n",
    "    final_docs = []\n",
    "    study_count = defaultdict(int)\n",
    "    max_per_study = max(2, k // 5)  # At least 2 docs per study, but limit dominance\n",
    "    \n",
    "    for doc in candidates_sorted:\n",
    "        study = doc.metadata.get('study', 'Unknown')\n",
    "        doc_type = doc.metadata.get('doc_type', '')\n",
    "        \n",
    "        # Always include high-priority docs\n",
    "        if doc.metadata.get('priority', 3) <= 1:\n",
    "            final_docs.append(doc)\n",
    "            study_count[study] += 1\n",
    "        # For lower priority, enforce study diversity\n",
    "        elif study_count[study] < max_per_study:\n",
    "            final_docs.append(doc)\n",
    "            study_count[study] += 1\n",
    "        \n",
    "        if len(final_docs) >= k:\n",
    "            break\n",
    "    \n",
    "    # If we don't have enough, add remaining candidates\n",
    "    if len(final_docs) < k:\n",
    "        for doc in candidates_sorted:\n",
    "            if doc not in final_docs:\n",
    "                final_docs.append(doc)\n",
    "            if len(final_docs) >= k:\n",
    "                break\n",
    "    \n",
    "    return final_docs[:k]\n",
    "\n",
    "print(\"‚úÖ Advanced retrieval function defined\")\n",
    "print(\"   Features:\")\n",
    "print(\"   ‚Ä¢ MMR for diversity across studies\")\n",
    "print(\"   ‚Ä¢ Priority-aware re-ranking (study summaries > subjects)\")\n",
    "print(\"   ‚Ä¢ Study balancing to prevent single-study dominance\")\n",
    "print(\"   ‚Ä¢ Optional study filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9fe818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG Query function 'ask()' is ready!\n",
      "\n",
      "Usage examples:\n",
      "  ask(\"What are the critical issues across all studies?\")\n",
      "  ask(\"Tell me about Study 10\", study_filter=\"Study 10\")\n",
      "  ask(\"Which studies have the worst data quality?\")\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: RAG Query Function with Humanized Prompt\n",
    "# ============================================================================\n",
    "\n",
    "# Humanized, detailed prompt for clinical trial assistant\n",
    "RAG_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert Clinical Trial Data Analyst and CRA (Clinical Research Associate) Assistant. \n",
    "Your role is to help clinical research teams understand their trial data, identify issues, and prioritize actions.\n",
    "\n",
    "**Your Communication Style:**\n",
    "- Be conversational yet professional\n",
    "- Provide thorough, detailed explanations\n",
    "- Use clinical research terminology appropriately\n",
    "- Highlight concerning patterns and offer insights\n",
    "- Be proactive in identifying potential issues\n",
    "\n",
    "**Response Structure:**\n",
    "\n",
    "## Summary\n",
    "A brief, friendly overview of your key findings (2-3 sentences).\n",
    "\n",
    "## Detailed Analysis\n",
    "Provide a comprehensive analysis of the data. Explain:\n",
    "- What the data shows\n",
    "- Why it matters for the clinical trial\n",
    "- Any patterns, trends, or anomalies you notice\n",
    "- Comparisons across studies/sites if relevant\n",
    "\n",
    "## Key Findings\n",
    "‚Ä¢ Bullet point the most important discoveries\n",
    "‚Ä¢ Include specific numbers and metrics\n",
    "‚Ä¢ Highlight any concerning issues\n",
    "\n",
    "## Recommended Actions\n",
    "Prioritized list of actions the CRA team should take:\n",
    "1. Immediate actions (safety/critical issues)\n",
    "2. Short-term actions (data quality)\n",
    "3. Monitoring recommendations\n",
    "\n",
    "## Sources Referenced\n",
    "List the specific documents and studies that informed your analysis.\n",
    "\n",
    "**Important Guidelines:**\n",
    "- Only use information from the provided context\n",
    "- If information is not available, clearly state this\n",
    "- Be specific with numbers, percentages, and study names\n",
    "- If asked about a specific study, focus on that study but provide cross-study context when relevant\n",
    "\n",
    "---\n",
    "**Context (Clinical Trial Data):**\n",
    "{context}\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def format_docs_with_metadata(docs):\n",
    "    \"\"\"Format documents with source information for better context.\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        study = doc.metadata.get('study', 'Unknown')\n",
    "        doc_type = doc.metadata.get('doc_type', 'Unknown')\n",
    "        \n",
    "        header = f\"[Document {i} | {doc_type} | {study}]\"\n",
    "        formatted.append(f\"{header}\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n---\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def ask(question: str, study_filter: str = None, k: int = 12, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Ask a question about clinical trial data using RAG.\n",
    "    \n",
    "    Args:\n",
    "        question: Your question about the clinical trial data\n",
    "        study_filter: Optional - filter to a specific study (e.g., \"Study 10\")\n",
    "        k: Number of documents to retrieve (default: 12)\n",
    "        verbose: Whether to print sources (default: True)\n",
    "    \n",
    "    Examples:\n",
    "        ask(\"What are the main issues across all studies?\")\n",
    "        ask(\"What's the status of Study 16?\", study_filter=\"Study 16\")\n",
    "        ask(\"Which sites have the most critical subjects?\")\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üîç Question: {question}\")\n",
    "        if study_filter:\n",
    "            print(f\"üìÅ Study Filter: {study_filter}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    docs = advanced_retrieve(question, k=k, study_filter=study_filter)\n",
    "    \n",
    "    if not docs:\n",
    "        print(\"‚ùå No relevant documents found.\")\n",
    "        return\n",
    "    \n",
    "    # Format context\n",
    "    context = format_docs_with_metadata(docs)\n",
    "    \n",
    "    # Generate response\n",
    "    chain = RAG_PROMPT | model | StrOutputParser()\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"üìö Documents Retrieved:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Group by type for cleaner output\n",
    "        by_type = defaultdict(list)\n",
    "        for doc in docs:\n",
    "            doc_type = doc.metadata.get('doc_type', 'unknown')\n",
    "            study = doc.metadata.get('study', 'Unknown')\n",
    "            by_type[doc_type].append(study)\n",
    "        \n",
    "        for doc_type, studies in by_type.items():\n",
    "            study_counts = defaultdict(int)\n",
    "            for s in studies:\n",
    "                study_counts[s] += 1\n",
    "            study_str = \", \".join(f\"{s}({c})\" for s, c in study_counts.items())\n",
    "            print(f\"  ‚Ä¢ {doc_type}: {study_str}\")\n",
    "\n",
    "print(\"‚úÖ RAG Query function 'ask()' is ready!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print('  ask(\"What are the critical issues across all studies?\")')\n",
    "print('  ask(\"Tell me about Study 10\", study_filter=\"Study 10\")')\n",
    "print('  ask(\"Which studies have the worst data quality?\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0762a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cad4191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è  This cell is for loading pre-built vector stores.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Alternative - Load Existing Vector Store (Skip Cells 4-6)\n",
    "# ============================================================================\n",
    "# If you've already created the vector store, use this cell to load it directly\n",
    "\n",
    "# Uncomment the following to load an existing vector store:\n",
    "\"\"\"\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "VECTORSTORE_PATH = \"faiss_index_optimized\"\n",
    "# Or use the original: VECTORSTORE_PATH = \"faiss_index\"\n",
    "\n",
    "if os.path.exists(VECTORSTORE_PATH):\n",
    "    vector_store = FAISS.load_local(\n",
    "        VECTORSTORE_PATH,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded vector store from: {VECTORSTORE_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå Vector store not found at: {VECTORSTORE_PATH}\")\n",
    "    print(\"   Please run cells 4-6 to create it first.\")\n",
    "\"\"\"\n",
    "print(\"‚ÑπÔ∏è  This cell is for loading pre-built vector stores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d346b",
   "metadata": {},
   "source": [
    "## üéØ Query Your Clinical Trial Data\n",
    "\n",
    "Run the cells below to ask questions about your clinical trial data.\n",
    "Modify the question and run the cell to get AI-powered insights.\n",
    "\n",
    "### Example Questions:\n",
    "- \"What are the most critical issues across all studies?\"\n",
    "- \"Give me a detailed analysis of Study 16\"\n",
    "- \"Which sites have the highest risk subjects?\"\n",
    "- \"Compare data quality between Study 1 and Study 10\"\n",
    "- \"What are the pending coding items that need attention?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14cd5cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç Question: What are the most critical data quality issues across all studies? Provide a summary with specific numbers.\n",
      "================================================================================\n",
      "\n",
      "## Summary\n",
      "The data quality reports reveal significant concerns with safety discrepancies and missing labs across multiple studies. Specifically, STUDY 24 and Study 7 have particularly low Data Quality Index (DQI) scores, indicating substantial data integrity risks. Addressing these issues is crucial for maintaining the reliability of the clinical trial data.\n",
      "\n",
      "## Detailed Analysis\n",
      "The provided data consists of Data Quality Index (DQI) reports for individual subjects across several studies (Study 1, Study 7, Study 10, Study 17, Study 22, Study 24, Study 25, and STUDY 2). The DQI score ranges from 0.0 to 98.0, with a status of ‚ÄúIssues Present‚Äù for all subjects reviewed. \n",
      "\n",
      "The primary drivers of low DQI scores appear to be *Safety Discrepancies* and *Missing Labs*. Other contributing factors include *Open Issues*, *Missing Pages*, and *MedDRA Pending* items. \n",
      "\n",
      "**Study-Specific Observations:**\n",
      "\n",
      "*   **STUDY 24:** Stands out with two subjects (11 and 278) having extremely low DQI scores (0.0/100 and 42.0/100 respectively). Subject 11 has 14 safety discrepancies and 51 missing labs, while Subject 278 has 58 missing labs.\n",
      "*   **Study 7:** Also demonstrates significant issues, with subjects 28 and 25 having DQI scores of 0.0/100 and 49.0/100. Subject 28 has 5 safety discrepancies and a very high number of missing labs (116), and Subject 25 has 51 missing labs.\n",
      "*   **Study 1:** Shows a mixed profile. Subject 62 has a relatively higher DQI (82.0/100), while Subject 47 has a DQI of 0.0/100, driven by 25 safety discrepancies and 13 missing pages.\n",
      "*   **Other Studies:** Study 10, Study 17, Study 22, Study 25 generally have higher DQI scores (ranging from 94.0/100 to 98.0/100), but still present with some issues, primarily related to missing labs or open issues.\n",
      "*   **STUDY 2:** Subject 111 has a DQI of 90.0/100 with 2 safety discrepancies.\n",
      "\n",
      "The consistent presence of ‚ÄúIssues Present‚Äù status across all subjects, even those with high DQI scores, suggests a systematic need for ongoing data quality monitoring and improvement.\n",
      "\n",
      "## Key Findings\n",
      "*   **Prevalence of Issues:** All reviewed subjects have data quality issues, as indicated by a DQI score below 100/100.\n",
      "*   **Critical Issue: Safety Discrepancies:** A significant concern, with counts ranging from 2 to 25 per subject. STUDY 24 and Study 7 have the highest number of safety discrepancies.\n",
      "*   **Dominant Issue: Missing Labs:** The most frequent issue, impacting nearly all subjects.  The highest counts are observed in STUDY 24 (51 & 58) and Study 7 (116 & 51).\n",
      "*   **Low DQI Scores:** Two subjects (Subject 11 in STUDY 24 and Subject 28 in Study 7) have a DQI of 0.0/100, indicating critical data quality failures.\n",
      "*   **Study-Specific Hotspots:** STUDY 24 and Study 7 demonstrate the most concerning patterns of data quality issues.\n",
      "\n",
      "## Recommended Actions\n",
      "1.  **Immediate Actions (Safety/Critical Issues):**\n",
      "    *   **STUDY 24 (Subject 11 & 278) & Study 7 (Subject 28):** Initiate an immediate investigation into the root cause of the high number of safety discrepancies and missing labs. This includes reviewing source documents, verifying data entry, and assessing potential risks to patient safety.\n",
      "    *   **All Studies:** Review all safety data for potential discrepancies, prioritizing subjects with lower DQI scores.\n",
      "2.  **Short-Term Actions (Data Quality):**\n",
      "    *   **STUDY 24 & Study 7:** Conduct a 100% source data verification (SDV) for these studies, focusing on safety data and lab results.\n",
      "    *   **All Studies:** Implement a targeted review of lab data to identify and resolve missing values.\n",
      "    *   **All Studies:** Review and address all open issues to improve data completeness.\n",
      "3.  **Monitoring Recommendations:**\n",
      "    *   **Increased Monitoring:** Increase the frequency of on-site monitoring for STUDY 24 and Study 7.\n",
      "    *   **Centralized Monitoring:** Implement centralized monitoring to identify trends in data quality issues across all studies.\n",
      "    *   **Training:** Provide additional training to site personnel on data entry, safety reporting, and documentation procedures.\n",
      "\n",
      "## Sources Referenced\n",
      "*   Document 1 | subject\\_dqi | STUDY 24\n",
      "*   Document 2 | subject\\_dqi | STUDY 22\n",
      "*   Document 3 | subject\\_dqi | Study 1\n",
      "*   Document 4 | subject\\_dqi | Study 1\n",
      "*   Document 5 | subject\\_dqi | Study 10\n",
      "*   Document 6 | subject\\_dqi | Study 7\n",
      "*   Document 7 | subject\\_dqi | STUDY 2\n",
      "*   Document 8 | subject\\_dqi | STUDY 24\n",
      "*   Document 9 | subject\\_dqi | Study 10\n",
      "*   Document 10 | subject\\_dqi | Study 7\n",
      "*   Document 11 | subject\\_dqi | Study 25\n",
      "*   Document 12 | subject\\_dqi | Study 17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üìö Documents Retrieved:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ subject_dqi: STUDY 24(2), STUDY 22(1), Study 1(2), Study 10(2), Study 7(2), STUDY 2(1), Study 25(1), Study 17(1)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# QUERY 1: Overview of all studies\n",
    "# ============================================================================\n",
    "\n",
    "ask(\"What are the most critical data quality issues across all studies? Provide a summary with specific numbers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb9dbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç Question: Tell me about Study 10. What are the key issues and which subjects need attention?\n",
      "üìÅ Study Filter: Study 10\n",
      "================================================================================\n",
      "\n",
      "## Summary\n",
      "\n",
      "Study 10 shows a moderate level of data issues with a significant proportion of subjects at medium risk. The primary concern is a high number of missing lab records, and a few subjects require immediate attention due to critical or high risk classifications.\n",
      "\n",
      "## Detailed Analysis\n",
      "\n",
      "Study 10 has a total of 59 subjects across 27 sites in 7 countries (USA, ITA, CHN, JPN, HUN...). A total of 147 issues have been identified. The risk distribution shows 19 subjects (32.2%) are low risk, 36 (61.0%) are medium risk, 3 (5.1%) are high risk, and 1 (1.7%) is critical risk. \n",
      "\n",
      "The issue breakdown reveals 3 open EDRR issues, no safety discrepancies, no missing CRF pages, 70 missing lab records, and 6 outstanding visits. The average number of issues per subject is 2.49, and 19 subjects have no issues at all. \n",
      "\n",
      "This study has a relatively high percentage of subjects with pending items (25.4%), with 15 subjects having outstanding data requirements.\n",
      "\n",
      "## Key Findings\n",
      "\n",
      "*   **Total Subjects:** 59\n",
      "*   **Total Issues:** 147\n",
      "*   **Average Issues per Subject:** 2.49\n",
      "*   **Critical Risk Subjects:** 1 (1.7%)\n",
      "*   **High Risk Subjects:** 3 (5.1%)\n",
      "*   **Missing Lab Records:** 70\n",
      "*   **Outstanding Visits:** 6\n",
      "*   **Subjects with Pending Items:** 15 (25.4%)\n",
      "\n",
      "## Recommended Actions\n",
      "\n",
      "1.  **Immediate Actions:** Focus on the 1 critical risk subject to ensure patient safety and data integrity.\n",
      "2.  **Short-term Actions:**\n",
      "    *   Resolve the 3 open EDRR issues to improve data quality.\n",
      "    *   Investigate and follow up on the 70 missing lab records ‚Äì this is the largest single issue in this study.\n",
      "3.  **Monitoring Recommendations:**\n",
      "    *   Prioritize site visits to address the 6 outstanding visits.\n",
      "    *   Review data for the 3 high-risk subjects to understand the source of their risk classification.\n",
      "\n",
      "## Sources Referenced\n",
      "\n",
      "*   Document 10 | study_summary | Study 10\n",
      "\n",
      "\n",
      "\n",
      "**Regarding which subjects need attention:**\n",
      "\n",
      "The study summary specifically highlights these subjects:\n",
      "\n",
      "*   **Subject 3509**\n",
      "*   **Subject 3517**\n",
      "*   **Subject 3521**\n",
      "*   **Subject 587**\n",
      "\n",
      "These subjects are flagged as requiring attention, likely due to their risk classification (high or critical). The CRA team should investigate the specific issues associated with these subjects first.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üìö Documents Retrieved:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ study_summary: Study 1(1), Study 2(1), Study 11(1), Study 22(1), Study 20(1), Study 5(1), Study 16(1), Study 19(1), Study 24(1), Study 10(1), Study 13(1), Study 6(1)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# QUERY 2: Study-specific analysis\n",
    "# ============================================================================\n",
    "\n",
    "ask(\"Tell me about Study 10. What are the key issues and which subjects need attention?\", \n",
    "    study_filter=\"Study 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae5d374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç Question: Which studies have the worst data quality and why? Compare the top 3 problematic studies.\n",
      "================================================================================\n",
      "\n",
      "## Summary\n",
      "The data reveals significant data quality concerns across multiple studies, with Studies 24, 7, and 1 exhibiting the lowest Data Quality Index (DQI) scores. These studies are primarily impacted by a high number of safety discrepancies and missing laboratory data, suggesting potential issues with data collection, reporting, or source verification.\n",
      "\n",
      "## Detailed Analysis\n",
      "We've analyzed Data Quality Index (DQI) reports for individual subjects across several studies. The DQI score provides a quantifiable measure of data quality, with lower scores indicating more issues. A score of 0/100 signifies critical data quality problems. \n",
      "\n",
      "The reports highlight several \"Quality Factors\" contributing to lower DQI scores:\n",
      "*   **Safety Discrepancies:** Discrepancies between reported adverse events and source documentation.\n",
      "*   **Missing Labs:** Incomplete laboratory data.\n",
      "*   **Open Issues:** Unresolved data queries or discrepancies.\n",
      "*   **WHODD Pending:** Adverse event coding awaiting WHO Drug Dictionary Enhanced (WHODD) review.\n",
      "*   **Missing Pages:** Missing documentation.\n",
      "*   **MedDRA Pending:** Adverse event coding awaiting Medical Dictionary for Regulatory Activities (MedDRA) review.\n",
      "\n",
      "Looking across the studies, there‚Äôs a wide range of DQI scores, indicating variable data quality. Some studies (e.g., STUDY 22, Study 10, Study 11) have subjects with relatively high DQI scores (90+/100) despite having some minor issues. However, others demonstrate substantial concerns.\n",
      "\n",
      "## Key Findings\n",
      "*   **Study 24** has the lowest DQI score of 0.0/100 for Subject 11, driven by 14 Safety Discrepancies and 51 Missing Labs. Subject 271 in Study 24 has a DQI of 62.5/100 with 1 Safety Discrepancy, 30 Missing Labs, and 5 WHODD Pending.\n",
      "*   **Study 7** has a DQI score of 0.0/100 for Subject 28, due to 5 Safety Discrepancies, 116 Missing Labs, and 3 MedDRA Pending. This represents the highest number of missing labs observed across all reports.\n",
      "*   **Study 1** has a DQI score of 0.0/100 for Subject 9, stemming from 29 Safety Discrepancies, 6 Missing Labs, 3 Missing Pages, and 1 Open Issue. This study has the highest number of safety discrepancies observed.\n",
      "*   **STUDY 22** has two reports with high DQI scores (98.0/100), but both have 1 Open Issue.\n",
      "*   **Study 10** has two reports with DQI scores of 97.5/100 and 94.0/100, with issues related to Missing Labs and WHODD Pending.\n",
      "*   **STUDY 2** has two reports with DQI scores of 90.0/100, both with 2 Safety Discrepancies.\n",
      "\n",
      "## Recommended Actions\n",
      "1.  **Immediate Actions (Safety/Critical Issues):**\n",
      "    *   **Study 24 (Subject 11), Study 7 (Subject 28), and Study 1 (Subject 9):** Initiate urgent safety review for these subjects due to the high number of safety discrepancies. Investigate the root cause of these discrepancies and ensure accurate reporting of adverse events.\n",
      "2.  **Short-Term Actions (Data Quality):**\n",
      "    *   **Study 7 (Subject 28):** Prioritize reconciliation of the 116 missing labs. This is a substantial data gap and could impact study conclusions.\n",
      "    *   **Study 24 (Subject 271) & Study 1 (Subject 9):** Investigate the reasons for the high number of missing labs and WHODD/MedDRA pending items. Implement training for site personnel on proper data collection and coding procedures.\n",
      "    *   **All Studies:** Review the data entry process for all sites to identify potential sources of error and implement quality control measures.\n",
      "3.  **Monitoring Recommendations:**\n",
      "    *   **Increased Monitoring:** Focus increased monitoring efforts on Studies 24, 7, and 1, with a particular emphasis on source data verification (SDV) for safety events and laboratory data.\n",
      "    *   **Centralized Review:** Consider implementing centralized review of safety data and lab results to identify discrepancies and ensure consistency.\n",
      "    *   **Training:** Provide refresher training to site staff on data quality expectations, adverse event reporting, and laboratory data handling.\n",
      "\n",
      "## Sources Referenced\n",
      "*   Document 1 | subject\\_dqi | STUDY 22\n",
      "*   Document 2 | subject\\_dqi | STUDY 24\n",
      "*   Document 3 | subject\\_dqi | Study 4\n",
      "*   Document 4 | subject\\_dqi | Study 10\n",
      "*   Document 5 | subject\\_dqi | Study 10\n",
      "*   Document 6 | subject\\_dqi | STUDY 2\n",
      "*   Document 7 | subject\\_dqi | STUDY 22\n",
      "*   Document 8 | subject\\_dqi | Study 7\n",
      "*   Document 9 | subject\\_dqi | STUDY 2\n",
      "*   Document 10 | subject\\_dqi | STUDY 24\n",
      "*   Document 11 | subject\\_dqi | Study 1\n",
      "*   Document 12 | subject\\_dqi | Study 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üìö Documents Retrieved:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ subject_dqi: STUDY 22(2), STUDY 24(2), Study 4(1), Study 10(2), STUDY 2(2), Study 7(1), Study 1(1), Study 11(1)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# QUERY 3: Cross-study comparison\n",
    "# ============================================================================\n",
    "\n",
    "ask(\"Which studies have the worst data quality and why? Compare the top 3 problematic studies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5955272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç Question: What are the safety discrepancies that need immediate attention?\n",
      "================================================================================\n",
      "\n",
      "## Summary\n",
      "The data reveals a concerning trend of numerous safety discrepancies across multiple subjects, particularly within Study 4. Subjects in Study 4 and Study 22 consistently exhibit a ‚ÄúCritical‚Äù risk category and a high number of pending safety reviews, demanding immediate attention from the CRA team.\n",
      "\n",
      "## Detailed Analysis\n",
      "The provided subject profiles highlight a significant focus on safety discrepancies. Several subjects across Studies 4, 22, and 25 have identified safety issues. \n",
      "\n",
      "**Study 4** stands out with a consistently ‚ÄúCritical‚Äù risk assessment for the majority of subjects reviewed (Subjects 1977, 1846, 1847, 1658, 1699, 2140, and 1156). The number of total issues ranges from 29 to 66 per subject, with a substantial number of discrepancies pending review (ranging from 1 to 12). This suggests potential systemic issues at the sites involved in Study 4.\n",
      "\n",
      "**Study 22** also presents concerns. Subjects 42593 and 42980 both have a ‚ÄúCritical‚Äù risk assessment. Subject 42593 has a notably high 36 pending safety discrepancies, while Subject 42980 has 3. \n",
      "\n",
      "**Study 25** appears to have fewer immediate concerns. Both subjects (34779 and 3989) have a ‚ÄúMedium‚Äù risk assessment, only 1 safety discrepancy identified, and no pending items requiring action.\n",
      "\n",
      "The geographic distribution of these issues is primarily within the USA for Studies 4 and 22, with one subject in Study 4 located in Poland and two subjects in Study 25 located in Singapore and Taiwan.\n",
      "\n",
      "## Key Findings\n",
      "*   **High Volume of Safety Discrepancies:** A total of 259 safety discrepancies have been identified across all subjects.\n",
      "*   **Study 4 is a Major Concern:** Subjects in Study 4 have the highest number of total issues (ranging from 29-66) and pending safety reviews (ranging from 1-12).\n",
      "*   **Critical Risk Predominance:** The majority of subjects with pending safety reviews are categorized as ‚ÄúCritical‚Äù risk.\n",
      "*   **Subject 42593 (Study 22) is High Priority:** This subject has the highest number of pending safety discrepancies (36).\n",
      "*   **Geographic Concentration:** The majority of critical risk subjects are located in the USA.\n",
      "\n",
      "## Recommended Actions\n",
      "1.  **Immediate Actions (Safety/Critical Issues):**\n",
      "    *   **Prioritize Review of Subject 42593 (Study 22):** The 36 pending safety discrepancies for this subject require *immediate* review and resolution.\n",
      "    *   **Initiate Urgent Safety Review for Study 4:** Given the consistently high number of discrepancies and ‚ÄúCritical‚Äù risk assessments, a focused safety review of Study 4 is essential.\n",
      "2.  **Short-Term Actions (Data Quality):**\n",
      "    *   **Investigate Sites with High Discrepancy Rates:** Focus on Sites 228, 211, 190, 245, and 133 (all involved in Study 4) to identify the root cause of the high number of safety discrepancies.\n",
      "    *   **Review Discrepancy Trends:** Analyze the *types* of safety discrepancies being reported to identify patterns and potential systemic issues.\n",
      "3.  **Monitoring Recommendations:**\n",
      "    *   **Increased Monitoring for Study 4 & 22:** Implement more frequent on-site or remote monitoring visits to the sites with the highest discrepancy rates.\n",
      "    *   **Training Reinforcement:** Consider targeted training for site staff on proper safety reporting procedures.\n",
      "\n",
      "## Sources Referenced\n",
      "*   Document 1 | subject_profile | Study 22\n",
      "*   Document 2 | subject_profile | Study 4\n",
      "*   Document 3 | subject_profile | Study 4\n",
      "*   Document 4 | subject_profile | Study 22\n",
      "*   Document 5 | subject_profile | Study 25\n",
      "*   Document 6 | subject_profile | Study 25\n",
      "*   Document 7 | subject_profile | Study 4\n",
      "*   Document 8 | subject_profile | Study 4\n",
      "*   Document 9 | subject_profile | Study 4\n",
      "*   Document 10 | subject_profile | Study 22\n",
      "*   Document 11 | subject_profile | Study 4\n",
      "*   Document 12 | subject_profile | Study 4\n",
      "\n",
      "\n",
      "\n",
      "**Regarding your question about safety discrepancies needing immediate attention:** Subject 42593 in Study 22 requires the most immediate attention due to the 36 pending safety discrepancies. Following that, a comprehensive review of all subjects in Study 4 is critical given the consistently high number of safety issues and \"Critical\" risk assessments.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üìö Documents Retrieved:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ subject_profile: Study 22(3), Study 4(7), Study 25(2)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# YOUR CUSTOM QUERY - Modify and run!\n",
    "# ============================================================================\n",
    "\n",
    "# Change the question below to ask anything about your clinical trial data:\n",
    "ask(\"What are the safety discrepancies that need immediate attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9189240f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç Question: Generate a CRA report for study 21\n",
      "================================================================================\n",
      "\n",
      "## Summary\n",
      "\n",
      "Study 21 demonstrates excellent overall data quality with a 99.2% clean data rate. However, there's a notable backlog of outstanding visits (354) and WHODD coding pending (136) that require attention. One subject has been flagged as High Risk and requires close monitoring.\n",
      "\n",
      "## Detailed Analysis\n",
      "\n",
      "The data for Study 21, as of December 22, 2025, indicates a very high level of data cleanliness. With 20,401 enrolled subjects, only 166 (0.8%) have identified issues. The lack of critical risk subjects is encouraging. The primary issues stem from outstanding visits and pending WHODD coding, suggesting potential delays in data processing and review rather than fundamental data errors. Open EDRR issues are present in 85 subjects. \n",
      "\n",
      "Compared to other studies reviewed (Study 4 with 34.9% clean data rate, Study 19 with 50.0%), Study 21 significantly outperforms in data quality. The average issues per subject are also remarkably low (0.0), contrasting sharply with Study 17 (6.3) and Study 7 (41.7). The absence of safety discrepancies is also a positive indicator.\n",
      "\n",
      "## Key Findings\n",
      "\n",
      "*   **Clean Data Rate:** 99.2% (20,235/20,401 subjects)\n",
      "*   **Subjects with Issues:** 166 (0.8%)\n",
      "*   **High Risk Subjects:** 1 (Subject 28392 with 8 issues)\n",
      "*   **Outstanding Visits:** 354 across 351 subjects\n",
      "*   **WHODD Coding Pending:** 136 across 81 subjects\n",
      "*   **Open EDRR Issues:** 93 across 85 subjects\n",
      "\n",
      "## Recommended Actions\n",
      "\n",
      "1.  **Immediate Actions (Safety/Critical Issues):** None required, as there are no critical risk subjects or open safety discrepancies.\n",
      "2.  **Short-Term Actions (Data Quality - within 2 weeks):**\n",
      "    *   **Visit Tracking:** Prioritize resolution of the 354 outstanding visits. Investigate the root cause of this backlog ‚Äì is it a site-specific issue, a data entry bottleneck, or a query resolution delay?\n",
      "    *   **WHODD Coding:** Expedite the completion of the 136 pending WHODD coding items. This is crucial for accurate adverse event reporting.\n",
      "    *   **EDRR Issues:** Follow up on the 93 open EDRR issues.\n",
      "3.  **Monitoring Recommendations:**\n",
      "    *   **High Risk Subject (Subject 28392):** Conduct a focused review of this subject‚Äôs data to understand the nature of the 8 issues and ensure they are resolved promptly.\n",
      "    *   **Site Performance:** Analyze the Site Performance Summary (Document 12) to identify sites with a higher number of issues and provide targeted support. Specifically, sites 1602, 1468, 215, 1052, 859, 1044, 1355, 786, 1420, and 1370 should be contacted.\n",
      "    *   **Trend Monitoring:** Continue to monitor these metrics closely in subsequent reports to ensure the high level of data quality is maintained.\n",
      "\n",
      "## Sources Referenced\n",
      "\n",
      "*   Document 12 | cra\\_report | Study 21 (CRA Monitoring Report for Study 21, dated 2025-12-22)\n",
      "*   All other CRA reports provided for context (Studies 2, 4, 7, 13, 15, 17, 18, 19, 20, 23) ‚Äì used for comparative analysis.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üìö Documents Retrieved:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ cra_report: Study 14(1), Study 19(1), Study 20(1), Study 18(1), Study 7(1), Study 23(1), Study 13(1), Study 17(1), Study 15(1), Study 2(1), Study 4(1), Study 21(1)\n"
     ]
    }
   ],
   "source": [
    "ask(\"Generate a CRA report for study 21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295d7a4",
   "metadata": {},
   "source": [
    "## üåê FastAPI Backend for Web Integration\n",
    "\n",
    "The following cells create a FastAPI server that:\n",
    "1. Accepts questions from the React frontend\n",
    "2. Streams responses using Server-Sent Events (SSE)\n",
    "3. Provides API endpoints for the dashboard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e6fe5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL: Install FastAPI Dependencies\n",
    "# ============================================================================\n",
    "# !pip install fastapi uvicorn python-multipart sse-starlette -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4567e38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FastAPI app created with endpoints:\n",
      "   GET  /                     - Health check\n",
      "   GET  /api/health           - API health status\n",
      "   POST /api/chat             - Chat with RAG (non-streaming)\n",
      "   POST /api/chat/stream      - Chat with RAG (streaming SSE)\n",
      "   GET  /api/dashboard        - Dashboard KPIs\n",
      "   GET  /api/studies          - List all studies\n",
      "   GET  /api/studies/{id}     - Study details\n",
      "   GET  /api/sites            - List all sites\n",
      "   GET  /api/subjects         - Subject records with status\n",
      "   GET  /api/ml-results       - ML model results & strategy\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL: FastAPI Backend with Streaming RAG Responses\n",
    "# ============================================================================\n",
    "# This creates a backend server that the React frontend will connect to\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Clinical Trial RAG API\",\n",
    "    description=\"API for clinical trial data analysis with AI-powered insights\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Enable CORS for React frontend\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\", \"http://localhost:5173\", \"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Request/Response Models\n",
    "class ChatRequest(BaseModel):\n",
    "    question: str\n",
    "    study_filter: Optional[str] = None\n",
    "    k: int = 12\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    sources: list\n",
    "\n",
    "# ============================================================================\n",
    "# API Endpoints\n",
    "# ============================================================================\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"status\": \"healthy\", \"message\": \"Clinical Trial RAG API is running\"}\n",
    "\n",
    "@app.get(\"/api/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"ok\", \"vector_store_loaded\": vector_store is not None}\n",
    "\n",
    "# Non-streaming chat endpoint\n",
    "@app.post(\"/api/chat\")\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"\n",
    "    Process a question and return the AI-generated answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve documents\n",
    "        docs = advanced_retrieve(\n",
    "            question=request.question,\n",
    "            k=request.k,\n",
    "            study_filter=request.study_filter\n",
    "        )\n",
    "        \n",
    "        if not docs:\n",
    "            raise HTTPException(status_code=404, detail=\"No relevant documents found\")\n",
    "        \n",
    "        # Format context\n",
    "        context = format_docs_with_metadata(docs)\n",
    "        \n",
    "        # Generate response\n",
    "        chain = RAG_PROMPT | model | StrOutputParser()\n",
    "        response = chain.invoke({\n",
    "            \"context\": context,\n",
    "            \"question\": request.question\n",
    "        })\n",
    "        \n",
    "        # Extract sources\n",
    "        sources = []\n",
    "        seen = set()\n",
    "        for doc in docs:\n",
    "            source_info = {\n",
    "                \"doc_type\": doc.metadata.get(\"doc_type\", \"unknown\"),\n",
    "                \"study\": doc.metadata.get(\"study\", \"Unknown\"),\n",
    "                \"source\": doc.metadata.get(\"source\", \"Unknown\")\n",
    "            }\n",
    "            source_key = f\"{source_info['study']}_{source_info['doc_type']}\"\n",
    "            if source_key not in seen:\n",
    "                sources.append(source_info)\n",
    "                seen.add(source_key)\n",
    "        \n",
    "        return ChatResponse(answer=response, sources=sources)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Streaming chat endpoint\n",
    "@app.post(\"/api/chat/stream\")\n",
    "async def chat_stream(request: ChatRequest):\n",
    "    \"\"\"\n",
    "    Stream the AI-generated response using Server-Sent Events.\n",
    "    \"\"\"\n",
    "    async def generate():\n",
    "        try:\n",
    "            # Retrieve documents\n",
    "            docs = advanced_retrieve(\n",
    "                question=request.question,\n",
    "                k=request.k,\n",
    "                study_filter=request.study_filter\n",
    "            )\n",
    "            \n",
    "            if not docs:\n",
    "                yield f\"data: {json.dumps({'error': 'No relevant documents found'})}\\n\\n\"\n",
    "                return\n",
    "            \n",
    "            # Format context\n",
    "            context = format_docs_with_metadata(docs)\n",
    "            \n",
    "            # Generate response (non-streaming from HuggingFace, but we chunk it for SSE)\n",
    "            chain = RAG_PROMPT | model | StrOutputParser()\n",
    "            full_response = chain.invoke({\n",
    "                \"context\": context,\n",
    "                \"question\": request.question\n",
    "            })\n",
    "            \n",
    "            # Stream response in chunks\n",
    "            chunk_size = 50\n",
    "            for i in range(0, len(full_response), chunk_size):\n",
    "                chunk = full_response[i:i+chunk_size]\n",
    "                yield f\"data: {json.dumps({'chunk': chunk, 'done': False})}\\n\\n\"\n",
    "                await asyncio.sleep(0.02)  # Small delay for streaming effect\n",
    "            \n",
    "            # Send sources at the end\n",
    "            sources = []\n",
    "            for doc in docs[:5]:\n",
    "                sources.append({\n",
    "                    \"doc_type\": doc.metadata.get(\"doc_type\", \"unknown\"),\n",
    "                    \"study\": doc.metadata.get(\"study\", \"Unknown\")\n",
    "                })\n",
    "            \n",
    "            yield f\"data: {json.dumps({'done': True, 'sources': sources})}\\n\\n\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            yield f\"data: {json.dumps({'error': str(e)})}\\n\\n\"\n",
    "    \n",
    "    return StreamingResponse(\n",
    "        generate(),\n",
    "        media_type=\"text/event-stream\",\n",
    "        headers={\n",
    "            \"Cache-Control\": \"no-cache\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Dashboard data endpoints\n",
    "@app.get(\"/api/dashboard\")\n",
    "async def get_dashboard_data():\n",
    "    \"\"\"Get main dashboard KPIs and overview data.\"\"\"\n",
    "    try:\n",
    "        with open(\"consolidated_data/dashboard_api.json\", \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/studies\")\n",
    "async def get_studies():\n",
    "    \"\"\"Get list of all studies with summaries.\"\"\"\n",
    "    try:\n",
    "        with open(\"consolidated_data/dashboard_api.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            return {\"studies\": data.get(\"studies\", [])}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/studies/{study_id}\")\n",
    "async def get_study_details(study_id: str):\n",
    "    \"\"\"Get detailed data for a specific study.\"\"\"\n",
    "    try:\n",
    "        # Load study summary\n",
    "        study_data = None\n",
    "        with open(\"consolidated_data/rag_study_documents.jsonl\", \"r\") as f:\n",
    "            for line in f:\n",
    "                doc = json.loads(line)\n",
    "                if doc.get(\"study\", \"\").lower() == study_id.lower():\n",
    "                    study_data = doc\n",
    "                    break\n",
    "        \n",
    "        if not study_data:\n",
    "            raise HTTPException(status_code=404, detail=f\"Study {study_id} not found\")\n",
    "        \n",
    "        return study_data\n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/sites\")\n",
    "async def get_sites():\n",
    "    \"\"\"Get list of all sites with performance data.\"\"\"\n",
    "    try:\n",
    "        sites = []\n",
    "        with open(\"consolidated_data/rag_site_documents.jsonl\", \"r\") as f:\n",
    "            for line in f:\n",
    "                doc = json.loads(line)\n",
    "                sites.append({\n",
    "                    \"id\": doc.get(\"id\"),\n",
    "                    \"study\": doc.get(\"study\"),\n",
    "                    \"site\": doc.get(\"site\"),\n",
    "                    \"total_subjects\": doc.get(\"total_subjects\", 0),\n",
    "                    \"total_issues\": doc.get(\"total_issues\", 0)\n",
    "                })\n",
    "        return {\"sites\": sites}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/ml-results\")\n",
    "async def get_ml_results():\n",
    "    \"\"\"Get ML model results, feature importance, and strategy details.\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Load ML models summary\n",
    "        with open(\"consolidated_data/ml_models_summary.json\", \"r\") as f:\n",
    "            models_summary = json.load(f)\n",
    "        \n",
    "        # Load feature importance\n",
    "        feature_df = pd.read_csv(\"consolidated_data/ml_feature_importance.csv\")\n",
    "        feature_importance = feature_df.to_dict('records')\n",
    "        \n",
    "        return {\n",
    "            \"models_summary\": models_summary,\n",
    "            \"feature_importance\": feature_importance,\n",
    "            \"ml_strategy\": {\n",
    "                \"title\": \"Clinical Trial Risk Prediction ML Pipeline\",\n",
    "                \"description\": \"A comprehensive machine learning approach to predict subject risk levels, pending items, and total issues in clinical trials.\",\n",
    "                \"tasks\": [\n",
    "                    {\n",
    "                        \"name\": \"Risk Classification\",\n",
    "                        \"type\": \"Multi-class Classification\",\n",
    "                        \"target\": \"risk_category (Low/Medium/High)\",\n",
    "                        \"description\": \"Predicts the risk category of each subject based on data quality indicators, coding completion rates, and issue counts.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Pending Items Classification\",\n",
    "                        \"type\": \"Binary Classification\", \n",
    "                        \"target\": \"has_pending_items (0/1)\",\n",
    "                        \"description\": \"Identifies subjects with pending coding items (MedDRA or WHO-DD) that require attention.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Issues Regression\",\n",
    "                        \"type\": \"Regression\",\n",
    "                        \"target\": \"total_issues (continuous)\",\n",
    "                        \"description\": \"Predicts the expected number of total issues for each subject to prioritize interventions.\"\n",
    "                    }\n",
    "                ],\n",
    "                \"models_used\": [\n",
    "                    {\"name\": \"Random Forest\", \"type\": \"Ensemble\", \"description\": \"Bagging-based ensemble with 100 decision trees for robust predictions.\"},\n",
    "                    {\"name\": \"Gradient Boosting\", \"type\": \"Ensemble\", \"description\": \"Sequential boosting algorithm that builds models iteratively to minimize errors.\"},\n",
    "                    {\"name\": \"Logistic Regression\", \"type\": \"Linear\", \"description\": \"Interpretable linear model for classification with regularization.\"},\n",
    "                    {\"name\": \"Ridge Regression\", \"type\": \"Linear\", \"description\": \"L2-regularized linear regression for issues prediction.\"}\n",
    "                ],\n",
    "                \"evaluation_metrics\": [\n",
    "                    {\"metric\": \"Accuracy\", \"description\": \"Proportion of correct predictions\"},\n",
    "                    {\"metric\": \"F1 Score\", \"description\": \"Harmonic mean of precision and recall\"},\n",
    "                    {\"metric\": \"Cross-Validation\", \"description\": \"5-fold CV to assess model generalization\"},\n",
    "                    {\"metric\": \"R¬≤ Score\", \"description\": \"Variance explained by regression models\"},\n",
    "                    {\"metric\": \"MAE/RMSE\", \"description\": \"Error metrics for regression tasks\"}\n",
    "                ],\n",
    "                \"features_used\": [\n",
    "                    \"open_issues_count\", \"safety_discrepancy_count\", \"missing_lab_count\",\n",
    "                    \"safety_completion_rate\", \"meddra_coding_pending\", \"whodd_coding_pending\",\n",
    "                    \"meddra_completion_rate\", \"whodd_completion_rate\", \"inactivated_forms_count\",\n",
    "                    \"outstanding_visits_count\", \"total_days_outstanding\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/subjects\")\n",
    "async def get_subjects():\n",
    "    \"\"\"Get all subjects with their status and predictions.\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        \n",
    "        df = pd.read_csv(\"consolidated_data/all_subjects_full.csv\")\n",
    "        \n",
    "        # Sample 1000 subjects for performance\n",
    "        if len(df) > 1000:\n",
    "            df = df.sample(n=1000, random_state=42)\n",
    "        \n",
    "        subjects = df.to_dict('records')\n",
    "        \n",
    "        # Get status distribution\n",
    "        status_dist = df['SubjectStatus'].value_counts().to_dict() if 'SubjectStatus' in df.columns else {}\n",
    "        risk_dist = df['risk_category'].value_counts().to_dict() if 'risk_category' in df.columns else {}\n",
    "        study_dist = df['Study'].value_counts().to_dict() if 'Study' in df.columns else {}\n",
    "        \n",
    "        return {\n",
    "            \"subjects\": subjects,\n",
    "            \"total_count\": len(pd.read_csv(\"consolidated_data/all_subjects_full.csv\")),\n",
    "            \"sample_count\": len(subjects),\n",
    "            \"status_distribution\": status_dist,\n",
    "            \"risk_distribution\": risk_dist,\n",
    "            \"study_distribution\": study_dist\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "print(\"‚úÖ FastAPI app created with endpoints:\")\n",
    "print(\"   GET  /                     - Health check\")\n",
    "print(\"   GET  /api/health           - API health status\")\n",
    "print(\"   POST /api/chat             - Chat with RAG (non-streaming)\")\n",
    "print(\"   POST /api/chat/stream      - Chat with RAG (streaming SSE)\")\n",
    "print(\"   GET  /api/dashboard        - Dashboard KPIs\")\n",
    "print(\"   GET  /api/studies          - List all studies\")\n",
    "print(\"   GET  /api/studies/{id}     - Study details\")\n",
    "print(\"   GET  /api/sites            - List all sites\")\n",
    "print(\"   GET  /api/subjects         - Subject records with status\")\n",
    "print(\"   GET  /api/ml-results       - ML model results & strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cce5d260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FastAPI server starting on http://localhost:8000\n",
      "üìñ API docs available at http://localhost:8000/docs\n",
      "\n",
      "‚ö†Ô∏è  Keep this notebook running while using the React frontend!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL: Run the FastAPI Server\n",
    "# ============================================================================\n",
    "# Run this cell to start the backend server on port 8000\n",
    "# The React frontend will connect to this server\n",
    "\n",
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"üöÄ FastAPI server starting on http://localhost:8000\")\n",
    "print(\"üìñ API docs available at http://localhost:8000/docs\")\n",
    "print(\"\\n‚ö†Ô∏è  Keep this notebook running while using the React frontend!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b49441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store type: <class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "Retrieved 1 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [5268]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "try:\n",
    "    if 'vector_store' in globals():\n",
    "        print(f\"Vector store type: {type(vector_store)}\")\n",
    "        docs = advanced_retrieve(\"test\", k=1)\n",
    "        print(f\"Retrieved {len(docs)} docs\")\n",
    "    else:\n",
    "        print(\"vector_store is NOT defined\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
